[api]
host = "0.0.0.0"
port = 8000
reload = true

[auth]
jwt_algorithm = "HS256"
dev_mode = true  # Enabled by default for easier local development
dev_entity_id = "dev_user"  # Default dev user ID
dev_entity_type = "developer"  # Default dev entity type
dev_permissions = ["read", "write", "admin"]  # Default dev permissions

#### Registered models
[registered_models]
azure_gpto4_mini = { model_name = "azure/o_series/o4-mini", api_base = "https://denis-macfrb0k-swedencentral.cognitiveservices.azure.com/", api_version = "2024-12-01-preview", deployment_id = "o4-mini" }
azure_embedding = { model_name = "azure/text-embedding-ada-002", api_base = "https://denis-macfrb0k-swedencentral.cognitiveservices.azure.com/", api_version = "2023-05-15", deployment_id = "text-embedding-ada-002" }
azure_router = { model_name = "azure/model-router", api_base = "https://denis-macfrb0k-swedencentral.cognitiveservices.azure.com/", api_version = "2024-12-01-preview", deployment_id = "model-router" }
azure_gpt_4_o = {model_name = "azure/o_series/gpt-4o", api_base="https://denis-macfrb0k-swedencentral.cognitiveservices.azure.com/", api_version = "2025-01-01-preview", deployment_id = "gpt-4o"}
google_gemini = {model_name = "vertex_ai/gemini-2.5-pro", vertex_project = "still-motif-450216-u2",  vertex_location = "europe-central2",  vertex_model= "gemini-2.5-pro" }


#### Component configurations ####

[agent]
model = "google_gemini" # Model for the agent logic

[completion]
model = "azure_gpt_4_o" #"openai_gpt4-1-mini"  # Reference to a key in registered_models
default_max_tokens = "5000"
default_temperature = 0.8

[database]
provider = "postgres"
# Connection pool settings
pool_size = 10           # Maximum number of connections in the pool
max_overflow = 15        # Maximum number of connections that can be created beyond pool_size
pool_recycle = 3600      # Time in seconds after which a connection is recycled (1 hour)
pool_timeout = 10        # Seconds to wait for a connection from the pool
pool_pre_ping = true     # Check connection viability before using it from the pool
max_retries = 3          # Number of retries for database operations
retry_delay = 1.0        # Initial delay between retries in seconds

[embedding]
model = "azure_embedding"  # Reference to registered model
dimensions = 1532
similarity_metric = "cosine"

[parser]
chunk_size = 6000
chunk_overlap = 600
use_unstructured_api = false
use_contextual_chunking = true
contextual_chunking_model = "google_gemini"  # Reference to a key in registered_models

[document_analysis]
model = "google_gemini"  # Reference to a key in registered_models

[parser.vision]
model = "azure_gpt_4_o"  # Reference to a key in registered_models
frame_sample_rate = -1  # Set to -1 to disable frame captioning

[reranker]
use_reranker = true
provider = "flag"
model_name = "BAAI/bge-reranker-large"
query_max_length = 256
passage_max_length = 512
use_fp16 = true
device = "mps" # use "cpu" if on docker and using a mac, "cuda" if cuda enabled device

[storage]
provider = "local"
storage_path = "./storage"

# [storage]
# provider = "aws-s3"
# region = "us-east-2"
# bucket_name = "morphik-s3-storage"

[vector_store]
provider = "pgvector"

[rules]
model = "google_gemini"
batch_size = 4096

[morphik]
enable_colpali = true
mode = "cloud"  # "cloud" or "self_hosted"
api_domain = "morphik.fintaxy.com"  # API domain for cloud URIs
# Only call the embedding API if colpali_mode is "api"
morphik_embedding_api_domain = "https://fintaxy--embeddings.modal.run"  # endpoint for multivector embedding service
colpali_mode = "api" # "off", "local", or "api"

[pdf_viewer]
frontend_url = "http://localhost:3000/api/pdf" # "https://morphik.ai/api/pdf" # "http://localhost:3000/api/pdf" # "https://morphik.ai/api/pdf"

[workflows]
model = "google_gemini"  # Model for workflow extraction tasks

[graph]
model = "google_gemini"
enable_entity_resolution = true

# [graph]
# mode="api"
# base_url="https://graph-api.morphik.ai"

[telemetry]
enabled = true
honeycomb_enabled = true
honeycomb_endpoint = "https://api.honeycomb.io"
honeycomb_proxy_endpoint = "https://otel-proxy.onrender.com"
service_name = "databridge-core"
otlp_timeout = 10
otlp_max_retries = 3
otlp_retry_delay = 1
otlp_max_export_batch_size = 512
otlp_schedule_delay_millis = 5000
otlp_max_queue_size = 2048
